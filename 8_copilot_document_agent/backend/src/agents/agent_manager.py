# src/agents/agent_manager.py

from typing import Dict, Any
from openai import OpenAI
from rich.console import Console
from src.retrieval.retriever import Retriever
from src.agents.tools import build_tools
from src.agents.prompts.classify_prompt import build_classify_prompt
from src.agents.prompts.final_prompt import build_final_prompt
from src.agents.prompts.tool_execution_prompt import build_tool_execution_prompt
import json

console = Console()


class AgentManager:
    """
    Gerencia o agente e suas ferramentas.
    Suporta racioc√≠nio multi-hop (v√°rias etapas):
    1Ô∏è‚É£ Classifica√ß√£o da pergunta
    2Ô∏è‚É£ Planejamento e execu√ß√£o de ferramentas
    3Ô∏è‚É£ Gera√ß√£o de resposta final com justificativas e cita√ß√µes
    """

    def __init__(self, retriever: Retriever, model_name: str = "gpt-4o-mini", client: OpenAI = None):
        self.retriever = retriever
        self.model_name = model_name
        self.client = client or OpenAI()
        self.tools = build_tools(self.retriever, shared_client=self.client)

    def _llm(self, prompt: str) -> str:
        """Fun√ß√£o utilit√°ria para gerar texto com o modelo LLM."""
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
        )
        return response.choices[0].message.content.strip()

    def ask(self, question: str) -> Dict[str, Any]:
        """
        Executa o racioc√≠nio multi-hop completo.
        """
        reasoning_steps = []
        tools_used = []

        console.rule("[bold green]üß† Iniciando racioc√≠nio multi-hop")
        console.print(f"[cyan]Pergunta:[/cyan] {question}")

        # 1Ô∏è‚É£ CLASSIFICA√á√ÉO
        reasoning_steps.append("üîç Analisando o tipo de pergunta...")
        console.print("[blue]üîπ Etapa 1:[/blue] Classificando tipo da pergunta...")

        classify_prompt = build_classify_prompt(question)
        analysis = self._llm(classify_prompt)
        reasoning_steps.append(f"üìò Classifica√ß√£o do LLM:\n{analysis}")

        try:
            parsed = json.loads(analysis)
            selected_tools = parsed.get("tools", [])
            reasoning_steps.append(f"üß∞ Ferramentas sugeridas: {', '.join(selected_tools)}")
        except Exception as e:
            console.print(f"[yellow]‚ö†Ô∏è Falha ao interpretar an√°lise ({e}), usando modo b√°sico (RAG).[/yellow]")
            selected_tools = ["basic_rag"]

        # 2Ô∏è‚É£ EXECU√á√ÉO DAS FERRAMENTAS
        console.print("[blue]üîπ Etapa 2:[/blue] Planejando e executando ferramentas...")
        reasoning_steps.append("‚öôÔ∏è Planejando execu√ß√£o das ferramentas...")

        final_context = ""

        if not selected_tools:
            reasoning_steps.append("‚ö†Ô∏è Nenhuma ferramenta sugerida ‚Äî executando basic_rag por padr√£o.")
            selected_tools = ["basic_rag"]

        # üß† Gera√ß√£o de plano de execu√ß√£o via LLM
        try:
            exec_prompt = build_tool_execution_prompt(question, selected_tools)
            exec_plan_raw = self._llm(exec_prompt)
            reasoning_steps.append(f"üóÇÔ∏è Plano de execu√ß√£o sugerido:\n{exec_plan_raw}")

            try:
                exec_plan = json.loads(exec_plan_raw)
            except Exception:
                exec_plan = [{"tool": t, "arguments": {"content": question}} for t in selected_tools]
                reasoning_steps.append("‚ö†Ô∏è Falha ao parsear plano, executando fallback simples.")
        except Exception as e:
            reasoning_steps.append(f"‚ùå Erro ao gerar plano de execu√ß√£o: {e}")
            exec_plan = [{"tool": t, "arguments": {"content": question}} for t in selected_tools]

        # üß© Executa cada ferramenta do plano
        for call in exec_plan:
            tool_name = call.get("tool")
            args = call.get("arguments", {})

            # Caso especial: basic_rag
            if tool_name == "basic_rag":
                reasoning_steps.append("üìñ Usando FAISS retriever para buscar contexto relevante...")
                console.print("[magenta]üîπ Ferramenta:[/magenta] basic_rag (FAISS retriever)")
                tools_used.append("faiss_retriever")

                results = self.retriever.search(question, top_k=3)
                contexts = [r["text"] for r in results]
                final_context += "\n\n".join(contexts)
                reasoning_steps.append(f"üîé {len(contexts)} contextos encontrados.")
                continue

            # Execu√ß√£o de ferramenta registrada
            if tool_name in self.tools:
                console.print(f"[magenta]üîπ Executando ferramenta:[/magenta] {tool_name}")
                tools_used.append(tool_name)

                try:
                    tool_func = self.tools[tool_name]
                    # LlamaIndex FunctionTool usa .fn para acessar a fun√ß√£o real
                    if hasattr(tool_func, "fn"):
                        func = tool_func.fn
                    else:
                        func = tool_func

                    # Passa o argumento principal (content ou question)
                    arg_value = args.get("content", question)
                    tool_result = func(arg_value)
                    final_context += f"\n\n[Trecho {tool_name}]\n{tool_result}"
                    reasoning_steps.append(f"‚úÖ {tool_name} executada com sucesso.")
                except Exception as e:
                    reasoning_steps.append(f"‚ùå Erro ao executar {tool_name}: {e}")
                    console.print(f"[red]Erro ao executar {tool_name}: {e}[/red]")
            else:
                reasoning_steps.append(f"‚ö†Ô∏è Ferramenta desconhecida: {tool_name}")
                tools_used.append(f"{tool_name} (n√£o encontrada)")
                console.print(f"[yellow]‚ö†Ô∏è Ferramenta desconhecida: {tool_name}[/yellow]")

        # 3Ô∏è‚É£ RESPOSTA FINAL
        reasoning_steps.append("üß† Gerando resposta final com base nas ferramentas executadas...")
        console.print("[blue]üîπ Etapa 3:[/blue] Gerando resposta final...")

        final_prompt = build_final_prompt(question, final_context)
        final_answer = self._llm(final_prompt)

        reasoning_steps.append("‚úÖ Resposta final gerada com sucesso.")
        console.rule("[bold green]üèÅ Racioc√≠nio conclu√≠do")

        return {
            "reasoning": "\n".join(reasoning_steps),
            "tools_used": tools_used,
            "final_answer": final_answer,
        }
